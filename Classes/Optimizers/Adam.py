# import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import variable_scope
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.framework import ops
from tensorflow.python.training import optimizer
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.eager import context
from tensorflow.python.eager import def_function
from tensorflow.python.framework import ops
from tensorflow.python.keras import backend_config
from tensorflow.python.keras.optimizer_v2 import optimizer_v2
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.training import gen_training_ops
from tensorflow.python.util.tf_export import keras_export
from Classes.Params import param



class Adam(keras.optimizers.Optimizer):

    _HAS_AGGREGATE_GRAD = True


    def __init__(self,
                learning_rate=0.001,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-7,
                amsgrad=False,
                name='Adam',
                **kwargs):

        #super().__init__(name, **kwargs)
        super(Adam, self).__init__(name, **kwargs)
        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
        self._set_hyper('decay', self._initial_decay)
        self._set_hyper('beta_1', beta_1)
        self._set_hyper('beta_2', beta_2)
        self.epsilon = epsilon or backend_config.epsilon()
        self.amsgrad = amsgrad

        # FedProx
        self._mu = param.MU
        self._mu_t = None
        print('learning_rate: {}'.format(learning_rate))
        self._set_hyper("mu", kwargs.get("mu", self._mu)) 

    def _create_slots(self, var_list):
        print('Created slots')

        # Create slots for the first and second moments.
        # Separate for-loops to respect the ordering of slot variables from v1.
        for var in var_list:
            self.add_slot(var, 'm')
        for var in var_list:
            self.add_slot(var, 'v')
        if self.amsgrad:
            for var in var_list:
                self.add_slot(var, 'vhat')
        # FedProx
        for var in var_list:
            self.add_slot(var, "vstar")



    def _prepare_local(self, var_device, var_dtype, apply_state):
        super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)

        local_step = math_ops.cast(self.iterations + 1, var_dtype)
        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))
        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))
        beta_1_power = math_ops.pow(beta_1_t, local_step)
        beta_2_power = math_ops.pow(beta_2_t, local_step)
        lr = (apply_state[(var_device, var_dtype)]['lr_t'] *
            (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
        apply_state[(var_device, var_dtype)].update(
            dict(
                lr=lr,
                epsilon=ops.convert_to_tensor_v2_with_dispatch(
                    self.epsilon, var_dtype),
                beta_1_t=beta_1_t,
                beta_1_power=beta_1_power,
                one_minus_beta_1_t=1 - beta_1_t,
                beta_2_t=beta_2_t,
                beta_2_power=beta_2_power,
                one_minus_beta_2_t=1 - beta_2_t))

    def set_weights(self, weights):
        params = self.weights
        # If the weights are generated by Keras V1 optimizer, it includes vhats
        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2
        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.
        num_vars = int((len(params) - 1) / 2)
        if len(weights) == 3 * num_vars + 1:
            weights = weights[:len(params)]
        super(Adam, self).set_weights(weights)

    def _resource_apply_dense(self, grad, var, apply_state=None):

        # FedProx
        vstar = self.get_slot(var, "vstar")
        self._mu_t = ops.convert_to_tensor(self._mu, name="prox_mu")
        mu_t = math_ops.cast(self._mu_t, var.dtype.base_dtype)
        grad = grad + mu_t*(var-vstar)

        var_device, var_dtype = var.device, var.dtype.base_dtype
        coefficients = ((apply_state or {}).get((var_device, var_dtype))
                        or self._fallback_apply_state(var_device, var_dtype))

        m = self.get_slot(var, 'm')
        v = self.get_slot(var, 'v')

        if not self.amsgrad:
            return gen_training_ops.ResourceApplyAdam(
                var=var.handle,
                m=m.handle,
                v=v.handle,
                beta1_power=coefficients['beta_1_power'],
                beta2_power=coefficients['beta_2_power'],
                lr=coefficients['lr_t'],
                beta1=coefficients['beta_1_t'],
                beta2=coefficients['beta_2_t'],
                epsilon=coefficients['epsilon'],
                grad=grad,
                use_locking=self._use_locking)
        else:
            vhat = self.get_slot(var, 'vhat')
            return gen_training_ops.ResourceApplyAdamWithAmsgrad(
                var=var.handle,
                m=m.handle,
                v=v.handle,
                vhat=vhat.handle,
                beta1_power=coefficients['beta_1_power'],
                beta2_power=coefficients['beta_2_power'],
                lr=coefficients['lr_t'],
                beta1=coefficients['beta_1_t'],
                beta2=coefficients['beta_2_t'],
                epsilon=coefficients['epsilon'],
                grad=grad,
                use_locking=self._use_locking)

    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):

        # FedProx
        vstar = self.get_slot(var, "vstar")
        mu_t = math_ops.cast(self._mu_t, var.dtype.base_dtype)
        grad = grad + mu_t*(var-vstar)

        var_device, var_dtype = var.device, var.dtype.base_dtype
        coefficients = ((apply_state or {}).get((var_device, var_dtype))
                        or self._fallback_apply_state(var_device, var_dtype))

        # m_t = beta1 * m + (1 - beta1) * g_t
        m = self.get_slot(var, 'm')
        m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']
        m_t = state_ops.assign(m, m * coefficients['beta_1_t'],
                            use_locking=self._use_locking)
        with ops.control_dependencies([m_t]):
            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)

        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)
        v = self.get_slot(var, 'v')
        v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']
        v_t = state_ops.assign(v, v * coefficients['beta_2_t'],
                            use_locking=self._use_locking)
        with ops.control_dependencies([v_t]):
            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)

        if not self.amsgrad:
            v_sqrt = math_ops.sqrt(v_t)
            var_update = state_ops.assign_sub(
            var, coefficients['lr'] * m_t / (v_sqrt + coefficients['epsilon']),
            use_locking=self._use_locking)
            return control_flow_ops.group(*[var_update, m_t, v_t])
        else:
            v_hat = self.get_slot(var, 'vhat')
            v_hat_t = math_ops.maximum(v_hat, v_t)
            with ops.control_dependencies([v_hat_t]):
                v_hat_t = state_ops.assign(
                v_hat, v_hat_t, use_locking=self._use_locking)
            v_hat_sqrt = math_ops.sqrt(v_hat_t)
            var_update = state_ops.assign_sub(
            var,
            coefficients['lr'] * m_t / (v_hat_sqrt + coefficients['epsilon']),
            use_locking=self._use_locking)
            return control_flow_ops.group(*[var_update, m_t, v_t, v_hat_t])


    # add slot for vstar
    def set_params(self, cog, client):
        all_vars = client.trainable_variables
        for variable, value in zip(all_vars, cog):
            vstar = self.get_slot(variable, "vstar")
            vstar.assign(value)


    def get_config(self):
        # base_config = super().get_config()
        # return {
        #     **base_config,
        #     'learning_rate': self._serialize_hyperparameter('learning_rate'),
        #     'decay': self._initial_decay,
        #     'beta_1': self._serialize_hyperparameter('beta_1'),
        #     'beta_2': self._serialize_hyperparameter('beta_2'),
        #     'epsilon': self.epsilon,
        #     'amsgrad': self.amsgrad,
        #     "mu": self._serialize_hyperparameter("mu"),
        # }

        config = super(Adam, self).get_config()
        config.update({
            'learning_rate': self._serialize_hyperparameter('learning_rate'),
            'decay': self._initial_decay,
            'beta_1': self._serialize_hyperparameter('beta_1'),
            'beta_2': self._serialize_hyperparameter('beta_2'),
            'epsilon': self.epsilon,
            'amsgrad': self.amsgrad,
            "mu": self._serialize_hyperparameter("mu")
        })
        return config


# def _resource_apply_dense(self, grad, var):
#     grad:     	tf.Tensor   dtype tf.float32
# self.lr:  	tf.Variable
# var:     	tf.Variable dtype tf.float32
# var.handle: tf.Tensor   dtype tf.resource


# # Tensor objects, numpy arrays, Python lists, and Python scalars -> Tensor objects
# self._lr_t = ops.convert_to_tensor(self._lr, name="learning_rate")

# # The operation casts x (in case of Tensor) or x.values (in case of SparseTensor or IndexedSlices) to dtype.
# lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)

# # Return a tf.Variable dtype tf.float32
# vstar = self.get_slot(var, "vstar")

# # Only apply to tf.Variable
# var_update = state_ops.assign_sub(var, lr_t*(grad + mu_t*(var-vstar)))
# var.assign(var_update)

# IN TF1 _apply_dense -> TF2 _resource_apply_dense
# return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t]) -> var.assign(var_update)